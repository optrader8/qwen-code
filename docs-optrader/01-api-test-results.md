# GPT-OSS-20B API Test Results

**Date**: 2025-11-09
**Server**: https://ryzen.parrot-mine.ts.net
**Model**: openai/gpt-4o (GPT-OSS-20B)

## Test Summary

All core features tested successfully:

- ‚úÖ Basic chat completion
- ‚úÖ Tool calling (function calling)
- ‚úÖ Streaming responses
- ‚úÖ Streaming + Tool calling combined

## Test 1: Basic Chat Completion

**Request:**

```bash
POST /v1/chat/completions
{
  "model": "openai/gpt-4o",
  "messages": [{"role": "user", "content": "Hello! What is 2+2?"}],
  "max_tokens": 100,
  "temperature": 0.7
}
```

**Response Structure:**

```json
{
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "message": {
        "role": "assistant",
        "reasoning_content": "The user: \"Hello! What is 2+2?\" Simple math. We answer 4. Should respond politely.",
        "content": "Hello! 2 + 2 equals **4**."
      }
    }
  ],
  "created": 1762694801,
  "model": "openai/gpt-4o",
  "system_fingerprint": "b6423-7057faf6",
  "object": "chat.completion",
  "usage": {
    "completion_tokens": 48,
    "prompt_tokens": 76,
    "total_tokens": 124
  },
  "timings": {
    "cache_n": 64,
    "prompt_n": 12,
    "prompt_ms": 102.481,
    "predicted_n": 48,
    "predicted_ms": 796.849
  }
}
```

**Key Findings:**

- OpenAI-compatible response format
- **New field**: `reasoning_content` - shows model's thinking process
- **New field**: `timings` - performance metrics (cache hits, inference time)
- Standard `usage` field for token counting

## Test 2: Tool Calling

**Request:**

```bash
{
  "model": "openai/gpt-4o",
  "messages": [{"role": "user", "content": "What is the weather in Seoul?"}],
  "tools": [{
    "type": "function",
    "function": {
      "name": "get_weather",
      "description": "Get the current weather in a given location",
      "parameters": {
        "type": "object",
        "properties": {
          "location": {"type": "string", "description": "The city name, e.g. Seoul"},
          "unit": {"type": "string", "enum": ["celsius", "fahrenheit"]}
        },
        "required": ["location"]
      }
    }
  }],
  "tool_choice": "auto"
}
```

**Response:**

```json
{
  "choices": [
    {
      "finish_reason": "tool_calls",
      "message": {
        "role": "assistant",
        "reasoning_content": "Need to call get_weather function.",
        "content": null,
        "tool_calls": [
          {
            "type": "function",
            "function": {
              "name": "get_weather",
              "arguments": "{\"location\":\"Seoul\",\"unit\":\"celsius\"}"
            },
            "id": "1mUUoGLCjj1gKA0AudroykLW7Ifi9NFC"
          }
        ]
      }
    }
  ]
}
```

**Key Findings:**

- Tool calling format identical to OpenAI
- `finish_reason`: "tool_calls"
- `tool_calls` array with `id`, `type`, `function.name`, `function.arguments`
- Arguments as JSON string (standard OpenAI format)

## Test 3: Streaming Response

**Request:**

```bash
{
  "model": "openai/gpt-4o",
  "messages": [{"role": "user", "content": "Count from 1 to 5"}],
  "stream": true,
  "max_tokens": 50
}
```

**Response Format:**

```
data: {"choices":[{"finish_reason":null,"index":0,"delta":{"role":"assistant","content":null}}],...}

data: {"choices":[{"finish_reason":null,"index":0,"delta":{"reasoning_content":"The"}}],...}

data: {"choices":[{"finish_reason":null,"index":0,"delta":{"reasoning_content":" user"}}],...}

...

data: {"choices":[{"finish_reason":"length","index":0,"delta":{}}],...}

data: {"choices":[],...,"usage":{...},"timings":{...}}

data: [DONE]
```

**Key Findings:**

- SSE (Server-Sent Events) format: `data: {json}`
- `object`: "chat.completion.chunk"
- `delta` structure for incremental updates
- **Streaming order**: `reasoning_content` streamed first, then `content`
- Last chunk contains `usage` and `timings`
- Ends with `data: [DONE]`

## Test 4: Streaming + Tool Calling

**Request:**

```bash
{
  "model": "openai/gpt-4o",
  "messages": [{"role": "user", "content": "What is the weather in Tokyo?"}],
  "tools": [...],
  "stream": true
}
```

**Response Flow:**

1. First chunks: `delta.reasoning_content` streamed incrementally
2. Tool call chunks:
   ```json
   {"delta": {"tool_calls": [{"index": 0, "id": "...", "type": "function", "function": {"name": "get_weather", "arguments": "{\""}}]}}
   {"delta": {"tool_calls": [{"index": 0, "function": {"arguments": "location"}}]}}
   {"delta": {"tool_calls": [{"index": 0, "function": {"arguments": "\":\""}}]}}
   {"delta": {"tool_calls": [{"index": 0, "function": {"arguments": "Tokyo"}}]}}
   ...
   ```
3. Final chunk with `finish_reason`: "tool_calls"

**Key Findings:**

- Tool calls streamed incrementally
- First chunk: `id`, `type`, `name`, start of `arguments`
- Subsequent chunks: `arguments` only (token by token)
- `index` field allows multiple tool calls

## GPT-OSS-20B Specific Features

### 1. `reasoning_content` Field

- **Purpose**: Shows the model's internal reasoning/thinking process
- **Location**: In `message` object (non-streaming) or `delta` (streaming)
- **When**: Always present, even for tool calls
- **Example**: "User asks \"What is the weather in Tokyo?\" We have a get_weather function. We should call it with location \"Tokyo\"."

### 2. `timings` Field

- **cache_n**: Number of cached tokens
- **prompt_n**: Number of prompt tokens processed
- **prompt_ms**: Prompt processing time in milliseconds
- **prompt_per_token_ms**: Average time per prompt token
- **prompt_per_second**: Tokens per second for prompt
- **predicted_n**: Number of tokens generated
- **predicted_ms**: Generation time in milliseconds
- **predicted_per_token_ms**: Average time per generated token
- **predicted_per_second**: Generation speed (tokens/sec)

## Compatibility Assessment

| Feature             | OpenAI Format | GPT-OSS-20B | Notes                   |
| ------------------- | ------------- | ----------- | ----------------------- |
| Chat Completion     | ‚úÖ            | ‚úÖ          | Fully compatible        |
| Tool Calling        | ‚úÖ            | ‚úÖ          | Identical structure     |
| Streaming           | ‚úÖ            | ‚úÖ          | SSE format, delta-based |
| Tool Call Streaming | ‚úÖ            | ‚úÖ          | Incremental arguments   |
| `reasoning_content` | ‚ùå            | ‚úÖ          | GPT-OSS-20B exclusive   |
| `timings`           | ‚ùå            | ‚úÖ          | GPT-OSS-20B exclusive   |

## Implementation Recommendations

### Phase 1: Configuration ‚úÖ

1. Token limits: Use 128K (already in tokenLimits.ts)
2. Add GPT-OSS-20B model constants to models.ts
3. No special API endpoint handling needed (OpenAI compatible)

### Phase 2: Core Changes üîß

1. **streamingToolCallParser.ts**:
   - Add support for `reasoning_content` in delta
   - Existing tool call parsing should work as-is

2. **converter.ts**:
   - Add `reasoning_content` to response types
   - Add `timings` to response metadata (optional)

3. **No breaking changes needed**: GPT-OSS-20B is a superset of OpenAI format

### Phase 3: Optional Enhancements üí°

1. Expose `reasoning_content` to users (debugging, transparency)
2. Use `timings` for performance monitoring
3. Create GPT-OSS-20B specific prompts if needed

## Next Steps

1. ‚úÖ API testing complete
2. üîß Add model configuration (models.ts)
3. üîß Update type definitions for `reasoning_content`
4. üîß Update streaming parser
5. ‚úÖ Test integration end-to-end
6. üìù Document configuration for users
